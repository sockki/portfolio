 #### **담당파트**
- 전체 페이지의 퍼블리싱 및 기능의 `40%` 정도 구현
- 사이트 상단과 하단의 **Header, Footer 컴포넌트 퍼블리싱 및 기능 개발**
- **보안 사고 문제 기사들을 확인 할 수 있는 "취약점 DB" 페이지 퍼블리싱 및 기능 구현 100% 기여**                     
    1. cnnvd 사이트 https://www.cnnvd.org.cn/home/warn 의 취약점 게시글을 `puppeteer` 라이브러리를 사용해 크롤링한 뒤에 `Llama3 AI` 를 사용해 번역한 후 `Firebase`에 저장                 
    2. 저장된 게시글 데이터를 한 페이지에 5개씩 보이도록 구현              
    3. **페이지네이션** 직접 구현           
    4. **상단의 3개의 배너**는 현재 게시물 중 가장 최신의 게시물 3개를 게시, 또한 마우스 포인터 위치에 따른 애니매이션 구현               
    5. `HOT`,`NEW` 버튼을 누르면 게시글을 최신순, 조회수 순으로 내림 차순 정렬 되도록 구현                
    6. 게시글에 붙은 라벨은 조회수 상위 10개 게시물일 경우 `HOT` 라벨, 48시간 내에 업데이트 된 게시물인 경우 `NEW`라벨이 붙도록 구현         
    7. 게시글의 **핀 아이콘**을 누르면 마이 페이지의 스크랩 된 게시물에서 확인 가능           
    8. 게시글의 **공유 아이콘**을 누르면 게시글 상세 페이지 링크를 공유 할 수 있음            
    9. **검색창**을 통해 게시글 제목을 필터링 하여 나열 할 수 있음              
    10. 우측의 **실시간 토픽**은 가장 많이 검색된 검색어를 내림 차순으로 나열               
    11. 비 로그인 상태일시 접근 할 수 없도록 **예외 처리** 구현                
    12. 페이지 로딩시에 **skeleton ui** 직접 구현하여 로딩 상태 구현                
- **보안 사고 기사글의 상세 내용을 볼 수 있는 "취약점 DB 상세 페이지" 퍼블리싱 및 기능 개발 90% 기여**                 
    1. 취약점 DB 페이지에서 게시글을 클릭하면 게시글의 **제목, 게시 날짜, 라벨, 내용** 등을 확인 할 수 있도록 구현                  
    2. 게시글 내용에 **테이블**이 존재 할 경우, 사용자가 데이터를 테이블 형식으로 볼 수 있도록 구현          
    3. **핀 아이콘과 공유 아이콘**을 통해 "취약점 DB" 페이지와 같이 스크랩, 공유 기능을 구현           
    4. 하단의 **비슷한 정보글**은 같은 회사에 관련된 정보글을 가져오도록 구현             
    5. 우측 하단의 버튼을 통해 게시들에 관련된 질의 응답이 가능 하도록 AI를 훈련하여 **QnA** 구현            
    6. 페이지 로딩시에 **skeleton ui** 직접 구현하여 로딩 상태 구현                    
    

#

#### **성과**
- 웅진씽크빅 [Next.js 프로젝트 캠프 2기] **우수상** 수상

#


#### 이슈 및 트러블 슈팅

 **1️⃣ puppeteer 라이브러리의 click 메소드와 setTimeout 함수를 통해 크롤링 타겟 페이지 내의 원하는 정보 얻어오기 구현**

**원인:**

- 회사측으로 부터 전달 받은 크롤링 타겟 페이지('https://www.cnnvd.org.cn/home/warn') 가 페이지 네이션, 글 상세 보기 와 같은 이벤트가 발생했을 때 url이 변경되지 않고 SPA 형식으로 이벤트가 처리 되는 것을 확인
- 이 때문에 url을 통해 페이지에 접근 하는 것이 아닌, 페이지와 상호작용을 통해 원하는 데이터에 접근 하는 방식을 고안할 필요성을 느낌

**해결**
- 다음과 같은 알고리즘을 통해 데이터를 크롤링 함         

❶ page.click()을 통해 기사의 상세 페이지에 접근함.           
❷ 각 요소의 class에 따라서 데이터를 title, uploadDate, Content 등을 key값으로 하여서 나누어서 Object 형식으로 변수에 저장                 
❸ page.click()을 통해 뒤로 가기 버튼을 클릭              
❹ 위의 3가지 과정을 for 문을 통해 반복하여 메인 페이지 내에 존재하는 모든 기사의 정보를 수집.                    
❺ page.click을 통해 pagination의 다음 페이지 버튼을 클릭                   
❻ for 문을 통해 (1,2,3),4 의 과정을 9 페이지 까지 반복. 총 90개의 기사 정보 저장                          
❼ 또한 page.click()을 진행 할 때마다, 요소 렌더링으로 인한 약 0.5초 ~ 1초 정도의 이벤트 블록이 발생 하므로 page.click() 이벤트 이후에 setTimeout 함수를 통해 딜레이를 발생 시킴                               

#


**2️⃣ 크롤링 데이터 번역 과정을 통해 발생하는 막대한 로딩 시간 확인. 사용자 로딩 시간 단축을 목적으로 크롤링 데이터 업데이트 방식을 변경**

**원인**:

- 최초에 회사의 기획에 의하면 매일 전체 크롤링 데이터를 가져와 캐싱하는 방식으로 구현 요청을 받음.
- 크롤링 타겟 사이트가 중국 사이트 이므로 데이터를 가져올 때 번역을 해서 저장 해야 하는데, 이 때 하나의 기사의 데이터를 전부 크롤링 한 뒤에 한꺼번에 번역하는 것이 로딩 시간이 덜 걸릴 것으로 생각(한번 번역 할 때 마다 약 3000~6000ms 소요)
- 그러나 Llama ai가 한꺼번에 번역 할 수 있는 데이터가 2024자 이므로 한번에 하나의 기사 전체 데이터를 번역 하는 것이 불가능 한 것을 확인(하나의 기사 당 약 20000자 이상의 글자 수)
- 때문에 데이터를 쪼개서 가져 온 뒤에 각각 번역 해야 함
- 하지만 이 방식으로 했을 때 하나의 기사를 전체 번역 후 저장 할 때마다 약 5분정도의 시간이 걸리는 것을 확인
- 때문에 100개의 기사를 가져올 경우 약 300분의 시간이 소요 되므로, 현재 방식에서는 매일 300분의 로딩 시간이 발생되는 것이 우려됨

**해결**:

- 데이터를 매일 매일 가져온 뒤 캐싱 하는 것이 아닌 파이어 베이스에 저장 하는 방식으로 변경
- 이후 매일 데이터를 업데이트 할 때 가장 최신의 기사보다 업로드 날짜라 빠르면 데이터를 가져오는 방식을 선택
- 이를 통해 매일 올라오는 게시글 수에 비례 하여서 로딩 시간을 가질 수 있으므로, 초기 계획에 비해서 로딩 시간을 매우 많이 단축

#

**3️⃣ 아티클 테이블 데이터를 firebase 형식에 어긋나지 않기 위해 객체 형식으로 저장**

**원인**
- 크롤링 해야 하는 데이터 중에 <td>, <tr> 태그로 이루어진 테이블 데이터가 존재함
- 처음에는 이 테이블 데이터를 크롤링 할 때 `string[][]` type 으로 저장했음
- 그러나 firebase는 중첩 배열을 지원 하지 않아서 다른 type으로 저장 해야 했음
- 이 때문에 데이터 저장 tool을 firebase가 아닌 다른 것으로 사용하려 했으나, 기업에서 firebase만을 사용하길 요청받음

**해결**
- 테이블 데이터를 `string[][]` type이 아닌 `object[]` type으로 저장 하는 것으로 결정
- 각 행의 데이터를 열의 개수에 따라 column1, column2, column3... 를 key로 하여서 object type으로 저장
- 이러한 방식으로 모든 행의 데이터를 수집해 배열 type으로 엮으면 테이블 데이터를 `object[]` type으로 저장 가능
- 이후 이렇게 저장한 object 데이터를 화면에 렌더링 할 때, 각 column들의 순서를 보장하기 위해 다음과 같은 배열을 사용해서 렌더링     
`const keyArray = ['column1','column2','column3','column4' ....]`

#

#### 성과 및 느낀 점

**1️⃣ 팀에서 컨밴션을 확실하게 정하고 코드 리뷰를 통해 서로의 코드를 보완하며 진행 하니 좋은 결과물이 나온 것 같다**
- 프로젝트 시작을 할 때 파일 이름, 함수 이름, type 작성, git 과 같은 대부분의 방면에서 컨밴션을 정하고 진행을 하였다. 
- 또한 pr이 올라오면 팀원들 모두 코드 리뷰를 철저히 진행을 하였다.
- 이러한 과정을 통해 대체적으로 코드들이 일관성이 있어서 서로의 컴포넌트들을 사용 및 수정 하기 쉬웠다. 또한 코드 리뷰 과정에서 내가 몰랐던 개발 방식에 대해 많이 공부 하게 된 점이 좋았다.
- 결과적으로 지금 까지 내가 진행했던 프로젝트 중에서 가장 진행 기간은 짧았지만, 코드 가독성 이나 UI/UX 와 같은 종합적인 면에서 가장 완성도 있는 프로젝트였으며 많은 것을 배울 수 있었다.

#

**2️⃣ 짧은 기간 내에 완성해야 하므로 성능을 고려 하지 않고 기능 구현을 한 점이 아쉽다.**
- firebase가 offset과 중간 키워드 검색 query를 제공하지 않아서 pagination과 검색 기능을 구현 할 때, 전체 데이터를 불러와서 처리하는 방식으로 구현됨
- 이 방법이 데이터베이스로 부터 필요한 데이터만을 가져 오는 것이 아닌, 전체 데이터를 가져오기 때문에 Firebase 사용량을 초과 할 수 있는 위험이 있다. 때문에 성능 개선이 필요하다고 생각했다.
- 그러나 시간 부족으로 인해 최종 발표 때 까지 개선하지 못했다. 때문에 이후에 리팩토링을 하면서 개선할 것으로 계획하였다.

#

**3️⃣ Next.js를 사용해 풀스택 개발을 진행해본 것이 새로운 경험이었다.**
- 이전까지 팀 프로젝트를 할 때는 백엔드로 부터 api를 제공 받으면, 그것을 client에서 처리하는 온전히 프론트엔드 작업만 진행했었다.
- 그러나 이번 프로젝트에서 내가 맡은 페이지는 데이터베이스 부터 내가 직접 설계하고 구현해야 했기 때문에 페이지에서 필요한 모든 것들을 0 부터 100까지 개발하는 풀스택 개발을 하였다.
- 직접 필요한 데이터를 구해서 저장하고, field를 설계 하며 '이것을 프론트에서 어떻게 처리하지' 와 같은 고민을 통해 백엔드에 대한 지식을 좀 더 얻어가는 프로젝트 였던 것 같다.